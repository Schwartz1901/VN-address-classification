{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i20WfB6lqiUy"
      },
      "outputs": [],
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# If you want to use your own database, download it here\n",
        "# !gdown ...\n",
        "\n",
        "# Provided text files\n",
        "# !gdown --fuzzy https://drive.google.com/file/d/1oSXQHLoVSGfBOLR4NjNwQRTkDb8Zd8OU/view?usp=drive_link -O list_province.txt\n",
        "# !gdown --fuzzy https://drive.google.com/file/d/18sZoDAqJWyUfmjQN3VpKfkDHFQ-tcml6/view?usp=drive_link -O list_district.txt\n",
        "# !gdown --fuzzy https://drive.google.com/file/d/1VfDCj7R11jf3SIZyoZdYL7fIN-AIhC-1/view?usp=drive_link -O list_ward.txt\n",
        "\n",
        "# Self - generated text files\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1STd4s-soelFToo67ejTNwFgA4ooErcuf/view?usp=sharing -O list_province.txt\n",
        "!gdown --fuzzy https://drive.google.com/file/d/16XmmpcCAAwZkcKlxhzSHErIHf2P9i10g/view?usp=sharing -O list_district.txt\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1x_vEugEAWgGEg-2rdKcGvj14HfxfEsXr/view?usp=sharing -O list_ward.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8znFuZTzwoS"
      },
      "outputs": [],
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# Add more to your needs\n",
        "# you must place ALL pip install here\n",
        "!pip install editdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AodaIxYa32hT"
      },
      "outputs": [],
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# import your library here\n",
        "import editdistance\n",
        "import re\n",
        "from collections import deque\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V25WQCLWVjP4"
      },
      "outputs": [],
      "source": [
        "# NOTE: you MUST change this cell\n",
        "# New methods / functions must be written under class Solution.\n",
        "\n",
        "class Solution:\n",
        "    def __init__(self):\n",
        "        # list provice, district, ward for private test, do not change for any reason (these file will be provided later with this exact name)\n",
        "\n",
        "        self.province_path = 'list_province.txt'\n",
        "        self.district_path = 'list_district.txt'\n",
        "        self.ward_path = 'list_ward.txt'\n",
        "\n",
        "        self.province_trie = MyTrie()\n",
        "        self.district_trie = MyTrie()\n",
        "        self.ward_trie = MyTrie()\n",
        "\n",
        "\n",
        "        self._create_trie(self.province_path, self.province_trie)\n",
        "        self._create_trie(self.district_path, self.district_trie)\n",
        "        self._create_trie(self.ward_path, self.ward_trie)\n",
        "\n",
        "       #     write your preprocess here, add more method if needed\n",
        "    def _segment(self, s: str):\n",
        "        ### Case 1: String contain \",\" as a delimiter\n",
        "        if (',' in s):\n",
        "            s = s.split(',')\n",
        "            s = [x.strip() for x in s]\n",
        "            return s\n",
        "        ### Case 2: String contain \".\" as a delimiter:\n",
        "        elif ('.' in s):\n",
        "            s = s.split('.')\n",
        "            s = [x.strip() for x in s]\n",
        "            return s\n",
        "        ### Case 3: String contain \" \" as a delimiter:\n",
        "        else:\n",
        "            # push a whole string into a list for the same output\n",
        "            s = [s]\n",
        "            return s\n",
        "\n",
        "    def _seperate(self, s: str):\n",
        "        # s = [re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', x) for x in s]\n",
        "        # s = [re.sub(r'([a-z])([A-Z])', r'\\1 \\2', x) for x in s]\n",
        "        s = [re.sub(r'([a-zA-Z])(?=[A-Z])', r'\\1 ', x) for x in s]\n",
        "\n",
        "        s = [re.sub(r'\\.', ' ', x) for x in s]\n",
        "        s = [re.sub(r'tp\\.?', '', x, flags=re.IGNORECASE) for x in s]\n",
        "\n",
        "        # s = [re.sub(r't\\.?', '', x, flags=re.IGNORECASE) for x in s]\n",
        "        # s = [re.sub(r'h\\.?', '', x, flags=re.IGNORECASE) for x in s]\n",
        "        return s\n",
        "\n",
        "    def _handle_prefix(self, s: str):\n",
        "        full_prefix = ['xã', 'thị trấn', 'phường', 'thị xã', 'huyện', 'tỉnh', 'thành phố']\n",
        "        regex = r'\\b(' + '|'.join(full_prefix) + r')\\.?\\s*(?=[A-Za-z0-9])'\n",
        "        s = re.sub(regex, '', s, flags=re.IGNORECASE).strip()\n",
        "        return s\n",
        "\n",
        "    def _generate_combination(self, s, max_com = 5):\n",
        "        words = s.split()\n",
        "        combinations = []\n",
        "        for length in range(2, max_com + 1):\n",
        "            for i in range(len(words) - length + 1):\n",
        "                combination = ' '.join(words[i:i + length])\n",
        "                combinations.append(combination)\n",
        "        return combinations\n",
        "\n",
        "        # Open Dataset\n",
        "    def _create_trie(self, dataset, trie):\n",
        "          with open(dataset, \"r\", encoding=\"utf-8\") as file:\n",
        "              for line in file:\n",
        "                  line = line.strip()\n",
        "                  trie.insert(line)\n",
        "\n",
        "    def _count_words(self, s: str):\n",
        "        \"\"\"Count the number of words in a string.\"\"\"\n",
        "        return len(s.split())\n",
        "\n",
        "\n",
        "    def process(self, s: str):\n",
        "        # Pre-process string\n",
        "        processed_str = self._seperate(self._segment(self._handle_prefix(s)))\n",
        "\n",
        "        # Initialize result dictionary\n",
        "        result = {\"province\": \"\", \"district\": \"\", \"ward\": \"\"}\n",
        "\n",
        "        # Early return if string is empty or too short\n",
        "        if not processed_str or len(processed_str) < 1:\n",
        "            return {\"cleaned_string\": processed_str, **result}\n",
        "\n",
        "        # Reverse once and work with list directly\n",
        "        address_parts = list(reversed(processed_str))\n",
        "\n",
        "        # Store matches with their distances\n",
        "        matches = {\n",
        "            \"province\": [],\n",
        "            \"district\": [],\n",
        "            \"ward\": []\n",
        "        }\n",
        "\n",
        "        # Process each address part\n",
        "        for address in address_parts:\n",
        "            word_count = self._count_words(address)\n",
        "            if word_count <= 1:\n",
        "                continue\n",
        "\n",
        "            # Optimize combination generation\n",
        "            combinations = (self._generate_combination(address, word_count)\n",
        "                        if word_count < 5\n",
        "                        else self._generate_combination(address))\n",
        "\n",
        "            # Search tries in a single pass\n",
        "            for combo in combinations:\n",
        "                for area_type, trie in [(\"province\", self.province_trie),\n",
        "                                    (\"district\", self.district_trie),\n",
        "                                    (\"ward\", self.ward_trie)]:\n",
        "                    if match := trie.search_closest(combo):\n",
        "                        distance = editdistance.eval(combo, match)\n",
        "                        matches[area_type].append((match, distance))\n",
        "                        break  # Move to next combination once matched\n",
        "\n",
        "        # Find best matches with minimum edit distance\n",
        "        for area_type in [\"province\", \"district\", \"ward\"]:\n",
        "            if matches[area_type]:\n",
        "                result[area_type] = min(matches[area_type], key=lambda x: x[1])[0]\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class MyTrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_word = False  # Will store the actual word when true\n",
        "\n",
        "class MyTrie:\n",
        "    def __init__(self):\n",
        "        self.root = MyTrieNode()\n",
        "\n",
        "    def insert(self, word):\n",
        "        \"\"\"Insert a word into the Trie.\"\"\"\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = MyTrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_word = word  # Store the complete word at the end node\n",
        "\n",
        "    def _collect_words_from_node(self, node, prefix, words, max_words=100):\n",
        "        \"\"\"Collect words efficiently using BFS with a limit.\"\"\"\n",
        "        queue = deque([(node, prefix)])\n",
        "        collected = 0\n",
        "\n",
        "        while queue and collected < max_words:\n",
        "            current_node, current_prefix = queue.popleft()\n",
        "            if current_node.is_word:\n",
        "                words.append(current_prefix)\n",
        "                collected += 1\n",
        "\n",
        "            for char, child in current_node.children.items():\n",
        "                queue.append((child, current_prefix + char))\n",
        "\n",
        "    def search_closest(self, query, max_distance=2):\n",
        "        \"\"\"Find the closest word to query within max_distance edits.\"\"\"\n",
        "        if not query:\n",
        "            return None\n",
        "\n",
        "        # Check for exact match\n",
        "        node = self.root\n",
        "        prefix = \"\"\n",
        "        for char in query:\n",
        "            if char not in node.children:\n",
        "                break\n",
        "            node = node.children[char]\n",
        "            prefix += char\n",
        "        else:  # Only executes if loop completes without breaking\n",
        "            if node.is_word:\n",
        "                return prefix\n",
        "\n",
        "        # Collect candidate words efficiently\n",
        "        candidate_words = []\n",
        "        self._collect_words_from_node(node, prefix, candidate_words)\n",
        "\n",
        "        # If no candidates found from current node, search from root\n",
        "        if not candidate_words:\n",
        "            self._collect_words_from_node(self.root, \"\", candidate_words)\n",
        "\n",
        "        if not candidate_words:\n",
        "            return None\n",
        "\n",
        "        # Find closest word with early termination\n",
        "        min_distance = float('inf')\n",
        "        closest_word = None\n",
        "\n",
        "        for word in candidate_words:\n",
        "            if word == query:  # Skip exact match (already checked)\n",
        "                continue\n",
        "            distance = editdistance.eval(query, word)\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                closest_word = word\n",
        "            if min_distance == 1:  # Can't get better than distance 1\n",
        "                break\n",
        "\n",
        "        # Return result based on criteria\n",
        "        return (closest_word\n",
        "                if (closest_word and\n",
        "                    min_distance <= max_distance and\n",
        "                    not closest_word.isdigit())\n",
        "                else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Sdb3ddTr1Jz"
      },
      "outputs": [],
      "source": [
        "# NOTE: DO NOT change this cell\n",
        "# This cell is for downloading private test\n",
        "!rm -rf test.json\n",
        "# this link is public test\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB/view?usp=sharing -O test.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KN8RZL6tFzI"
      },
      "outputs": [],
      "source": [
        "# CORRECT TESTS\n",
        "groups_province = {}\n",
        "groups_district = {'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'kbang': ['Kbang', 'KBang'], 'quy nhơn': ['Qui Nhơn', 'Quy Nhơn']}\n",
        "groups_ward = {'ái nghĩa': ['ái Nghĩa', 'Ái Nghĩa'], 'ái quốc': ['ái Quốc', 'Ái Quốc'], 'ái thượng': ['ái Thượng', 'Ái Thượng'], 'ái tử': ['ái Tử', 'Ái Tử'], 'ấm hạ': ['ấm Hạ', 'Ấm Hạ'], 'an ấp': ['An ấp', 'An Ấp'], 'ẳng cang': ['ẳng Cang', 'Ẳng Cang'], 'ẳng nưa': ['ẳng Nưa', 'Ẳng Nưa'], 'ẳng tở': ['ẳng Tở', 'Ẳng Tở'], 'an hòa': ['An Hoà', 'An Hòa'], 'ayun': ['Ayun', 'AYun'], 'bắc ái': ['Bắc ái', 'Bắc Ái'], 'bảo ái': ['Bảo ái', 'Bảo Ái'], 'bình hòa': ['Bình Hoà', 'Bình Hòa'], 'châu ổ': ['Châu ổ', 'Châu Ổ'], 'chư á': ['Chư á', 'Chư Á'], 'chư rcăm': ['Chư Rcăm', 'Chư RCăm'], 'cộng hòa': ['Cộng Hoà', 'Cộng Hòa'], 'cò nòi': ['Cò  Nòi', 'Cò Nòi'], 'đại ân 2': ['Đại Ân  2', 'Đại Ân 2'], 'đak ơ': ['Đak ơ', 'Đak Ơ'], \"đạ m'ri\": [\"Đạ M'ri\", \"Đạ M'Ri\"], 'đông hòa': ['Đông Hoà', 'Đông Hòa'], 'đồng ích': ['Đồng ích', 'Đồng Ích'], 'hải châu i': ['Hải Châu  I', 'Hải Châu I'], 'hải hòa': ['Hải Hoà', 'Hải Hòa'], 'hành tín đông': ['Hành Tín  Đông', 'Hành Tín Đông'], 'hiệp hòa': ['Hiệp Hoà', 'Hiệp Hòa'], 'hòa bắc': ['Hoà Bắc', 'Hòa Bắc'], 'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'hòa châu': ['Hoà Châu', 'Hòa Châu'], 'hòa hải': ['Hoà Hải', 'Hòa Hải'], 'hòa hiệp trung': ['Hoà Hiệp Trung', 'Hòa Hiệp Trung'], 'hòa liên': ['Hoà Liên', 'Hòa Liên'], 'hòa lộc': ['Hoà Lộc', 'Hòa Lộc'], 'hòa lợi': ['Hoà Lợi', 'Hòa Lợi'], 'hòa long': ['Hoà Long', 'Hòa Long'], 'hòa mạc': ['Hoà Mạc', 'Hòa Mạc'], 'hòa minh': ['Hoà Minh', 'Hòa Minh'], 'hòa mỹ': ['Hoà Mỹ', 'Hòa Mỹ'], 'hòa phát': ['Hoà Phát', 'Hòa Phát'], 'hòa phong': ['Hoà Phong', 'Hòa Phong'], 'hòa phú': ['Hoà Phú', 'Hòa Phú'], 'hòa phước': ['Hoà Phước', 'Hòa Phước'], 'hòa sơn': ['Hoà Sơn', 'Hòa Sơn'], 'hòa tân': ['Hoà Tân', 'Hòa Tân'], 'hòa thuận': ['Hoà Thuận', 'Hòa Thuận'], 'hòa tiến': ['Hoà Tiến', 'Hòa Tiến'], 'hòa trạch': ['Hoà Trạch', 'Hòa Trạch'], 'hòa vinh': ['Hoà Vinh', 'Hòa Vinh'], 'hương hòa': ['Hương Hoà', 'Hương Hòa'], 'ích hậu': ['ích Hậu', 'Ích Hậu'], 'ít ong': ['ít Ong', 'Ít Ong'], 'khánh hòa': ['Khánh Hoà', 'Khánh Hòa'], 'krông á': ['Krông Á', 'KRông á'], 'lộc hòa': ['Lộc Hoà', 'Lộc Hòa'], 'minh hòa': ['Minh Hoà', 'Minh Hòa'], 'mường ải': ['Mường ải', 'Mường Ải'], 'mường ẳng': ['Mường ẳng', 'Mường Ẳng'], 'nậm ét': ['Nậm ét', 'Nậm Ét'], 'nam hòa': ['Nam Hoà', 'Nam Hòa'], 'na ư': ['Na ư', 'Na Ư'], 'ngã sáu': ['Ngã sáu', 'Ngã Sáu'], 'nghi hòa': ['Nghi Hoà', 'Nghi Hòa'], 'nguyễn úy': ['Nguyễn Uý', 'Nguyễn úy', 'Nguyễn Úy'], 'nhân hòa': ['Nhân Hoà', 'Nhân Hòa'], 'nhơn hòa': ['Nhơn Hoà', 'Nhơn Hòa'], 'nhơn nghĩa a': ['Nhơn nghĩa A', 'Nhơn Nghĩa A'], 'phúc ứng': ['Phúc ứng', 'Phúc Ứng'], 'phước hòa': ['Phước Hoà', 'Phước Hòa'], 'sơn hóa': ['Sơn Hoá', 'Sơn Hóa'], 'tạ an khương đông': ['Tạ An Khương  Đông', 'Tạ An Khương Đông'], 'tạ an khương nam': ['Tạ An Khương  Nam', 'Tạ An Khương Nam'], 'tăng hòa': ['Tăng Hoà', 'Tăng Hòa'], 'tân hòa': ['Tân Hoà', 'Tân Hòa'], 'tân hòa thành': ['Tân Hòa  Thành', 'Tân Hòa Thành'], 'tân khánh trung': ['Tân  Khánh Trung', 'Tân Khánh Trung'], 'tân lợi': ['Tân lợi', 'Tân Lợi'], 'thái hòa': ['Thái Hoà', 'Thái Hòa'], 'thiết ống': ['Thiết ống', 'Thiết Ống'], 'thuận hòa': ['Thuận Hoà', 'Thuận Hòa'], 'thượng ấm': ['Thượng ấm', 'Thượng Ấm'], 'thụy hương': ['Thuỵ Hương', 'Thụy Hương'], 'thủy xuân': ['Thuỷ Xuân', 'Thủy Xuân'], 'tịnh ấn đông': ['Tịnh ấn Đông', 'Tịnh Ấn Đông'], 'tịnh ấn tây': ['Tịnh ấn Tây', 'Tịnh Ấn Tây'], 'triệu ái': ['Triệu ái', 'Triệu Ái'], 'triệu ẩu': ['Triệu ẩu', 'Triệu Ẩu'], 'trung hòa': ['Trung Hoà', 'Trung Hòa'], 'trung ý': ['Trung ý', 'Trung Ý'], 'tùng ảnh': ['Tùng ảnh', 'Tùng Ảnh'], 'úc kỳ': ['úc Kỳ', 'Úc Kỳ'], 'ứng hòe': ['ứng Hoè', 'Ứng Hoè'], 'vĩnh hòa': ['Vĩnh Hoà', 'Vĩnh Hòa'], 'vũ hòa': ['Vũ Hoà', 'Vũ Hòa'], 'xuân ái': ['Xuân ái', 'Xuân Ái'], 'xuân áng': ['Xuân áng', 'Xuân Áng'], 'xuân hòa': ['Xuân Hoà', 'Xuân Hòa'], 'xuất hóa': ['Xuất Hoá', 'Xuất Hóa'], 'ỷ la': ['ỷ La', 'Ỷ La']}\n",
        "groups_ward.update({1: ['1', '01'], 2: ['2', '02'], 3: ['3', '03'], 4: ['4', '04'], 5: ['5', '05'], 6: ['6', '06'], 7: ['7', '07'], 8: ['8', '08'], 9: ['9', '09']})\n",
        "def to_same(groups):\n",
        "    same = {ele: k for k, v in groups.items() for ele in v}\n",
        "    return same\n",
        "same_province = to_same(groups_province)\n",
        "same_district = to_same(groups_district)\n",
        "same_ward = to_same(groups_ward)\n",
        "def normalize(text, same_dict):\n",
        "    return same_dict.get(text, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjO6FFcA0DYi"
      },
      "outputs": [],
      "source": [
        "TEAM_NAME = 'GROUP_2'  # This should be your team name\n",
        "EXCEL_FILE = f'{TEAM_NAME}.xlsx'\n",
        "\n",
        "import json\n",
        "import time\n",
        "with open('test.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "summary_only = True\n",
        "df = []\n",
        "solution = Solution()\n",
        "timer = []\n",
        "correct = 0\n",
        "for test_idx, data_point in enumerate(data):\n",
        "    address = data_point[\"text\"]\n",
        "\n",
        "    ok = 0\n",
        "    try:\n",
        "        answer = data_point[\"result\"]\n",
        "        answer[\"province_normalized\"] = normalize(answer[\"province\"], same_province)\n",
        "        answer[\"district_normalized\"] = normalize(answer[\"district\"], same_district)\n",
        "        answer[\"ward_normalized\"] = normalize(answer[\"ward\"], same_ward)\n",
        "\n",
        "        start = time.perf_counter_ns()\n",
        "        result = solution.process(address)\n",
        "        finish = time.perf_counter_ns()\n",
        "        timer.append(finish - start)\n",
        "        result[\"province_normalized\"] = normalize(result[\"province\"], same_province)\n",
        "        result[\"district_normalized\"] = normalize(result[\"district\"], same_district)\n",
        "        result[\"ward_normalized\"] = normalize(result[\"ward\"], same_ward)\n",
        "\n",
        "        province_correct = int(answer[\"province_normalized\"] == result[\"province_normalized\"])\n",
        "        district_correct = int(answer[\"district_normalized\"] == result[\"district_normalized\"])\n",
        "        ward_correct = int(answer[\"ward_normalized\"] == result[\"ward_normalized\"])\n",
        "        ok = province_correct + district_correct + ward_correct\n",
        "\n",
        "        df.append([\n",
        "            test_idx,\n",
        "            address,\n",
        "            answer[\"province\"],\n",
        "            result[\"province\"],\n",
        "            answer[\"province_normalized\"],\n",
        "            result[\"province_normalized\"],\n",
        "            province_correct,\n",
        "            answer[\"district\"],\n",
        "            result[\"district\"],\n",
        "            answer[\"district_normalized\"],\n",
        "            result[\"district_normalized\"],\n",
        "            district_correct,\n",
        "            answer[\"ward\"],\n",
        "            result[\"ward\"],\n",
        "            answer[\"ward_normalized\"],\n",
        "            result[\"ward_normalized\"],\n",
        "            ward_correct,\n",
        "            ok,\n",
        "            timer[-1] / 1_000_000_000,\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        print(f\"{answer = }\")\n",
        "        print(f\"{result = }\")\n",
        "        df.append([\n",
        "            test_idx,\n",
        "            address,\n",
        "            answer[\"province\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"province_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            answer[\"district\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"district_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            answer[\"ward\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"ward_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            0,\n",
        "            0,\n",
        "        ])\n",
        "        # any failure count as a zero correct\n",
        "        pass\n",
        "    correct += ok\n",
        "\n",
        "\n",
        "    if not summary_only:\n",
        "        # responsive stuff\n",
        "        print(f\"Test {test_idx:5d}/{len(data):5d}\")\n",
        "        print(f\"Correct: {ok}/3\")\n",
        "        print(f\"Time Executed: {timer[-1] / 1_000_000_000:.4f}\")\n",
        "\n",
        "\n",
        "print(f\"-\"*30)\n",
        "total = len(data) * 3\n",
        "score_scale_10 = round(correct / total * 10, 2)\n",
        "if len(timer) == 0:\n",
        "    timer = [0]\n",
        "max_time_sec = round(max(timer) / 1_000_000_000, 4)\n",
        "avg_time_sec = round((sum(timer) / len(timer)) / 1_000_000_000, 4)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df2 = pd.DataFrame(\n",
        "    [[correct, total, score_scale_10, max_time_sec, avg_time_sec]],\n",
        "    columns=['correct', 'total', 'score / 10', 'max_time_sec', 'avg_time_sec',],\n",
        ")\n",
        "\n",
        "columns = [\n",
        "    'ID',\n",
        "    'text',\n",
        "    'province',\n",
        "    'province_student',\n",
        "    'province_normalized',\n",
        "    'province_student_normalized',\n",
        "    'province_correct',\n",
        "    'district',\n",
        "    'district_student',\n",
        "    'district_normalized',\n",
        "    'district_student_normalized',\n",
        "    'district_correct',\n",
        "    'ward',\n",
        "    'ward_student',\n",
        "    'ward_normalized',\n",
        "    'ward_student_normalized',\n",
        "    'ward_correct',\n",
        "    'total_correct',\n",
        "    'time_sec',\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "df.columns = columns\n",
        "\n",
        "print(f'{TEAM_NAME = }')\n",
        "print(f'{EXCEL_FILE = }')\n",
        "print(df2)\n",
        "\n",
        "!pip install xlsxwriter\n",
        "writer = pd.ExcelWriter(EXCEL_FILE, engine='xlsxwriter')\n",
        "df2.to_excel(writer, index=False, sheet_name='summary')\n",
        "df.to_excel(writer, index=False, sheet_name='details')\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
